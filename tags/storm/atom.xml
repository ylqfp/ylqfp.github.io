<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Tag: storm | LeoYu的技术博客]]></title>
  <link href="http://ylqfp.github.io/tags/storm/atom.xml" rel="self"/>
  <link href="http://ylqfp.github.io/"/>
  <updated>2015-06-14T09:59:22+08:00</updated>
  <id>http://ylqfp.github.io/</id>
  <author>
    <name><![CDATA[LeoYu]]></name>
    <email><![CDATA[ylqfp@qq.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[第一篇博客]]></title>
    <link href="http://ylqfp.github.io/blog/2015/06/13/first-blog/"/>
    <updated>2015-06-13T08:25:27+08:00</updated>
    <id>http://ylqfp.github.io/blog/2015/06/13/first-blog</id>
    <content type="html"><![CDATA[<h1>这是我的第一篇文章</h1>

<blockquote><p>知世故而不世故，知人性而包容万物。</p></blockquote>

<p>本博客包含以下技术内容：</p>

<h2>大数据&amp;&amp;并行</h2>

<ul>
<li>Hadoop</li>
<li>Spark</li>
<li>Storm

<h2>科研</h2></li>
<li>机器学习</li>
<li>自然语言处理</li>
<li>计算机视觉

<h2>JAVA代码</h2>

<p>``` java
import java.nio.channels.FileChannel;</p></li>
</ul>


<p>public interface CallbackInterface(){
    public void dothings();
    public void exeMethods();
}</p>

<p>public class Server1 {</p>

<pre><code>public static void main(String[] args) {
    // TODO Auto-generated method stub
    System.out.println("hello world!!!");
}
</code></pre>

<p>}</p>

<pre><code>## Python代码
</code></pre>

<p>import numpy as np
import scipy.sparse
import scipy.optimize</p>

<p>def softmax_cost(theta, num_classes, input_size, lambda<em>, data, labels):
    &ldquo;&rdquo;&ldquo;
    :param theta:
    :param num_classes: the number of classes
    :param input_size: the size N of input vector
    :param lambda</em>: weight decay parameter
    :param data: the N x M input matrix, where each column corresponds
                 a single test set
    :param labels: an M x 1 matrix containing the labels for the input data
    &rdquo;&ldquo;&rdquo;
    m = data.shape[1]
    theta = theta.reshape(num_classes, input_size)
    theta_data = theta.dot(data)
    theta_data = theta_data - np.max(theta_data)
    prob_data = np.exp(theta_data) / np.sum(np.exp(theta_data), axis=0)
    indicator = scipy.sparse.csr_matrix((np.ones(m), (labels, np.array(range(m)))))
    indicator = np.array(indicator.todense())
    cost = (-1 / m) * np.sum(indicator * np.log(prob_data)) + (lambda<em> / 2) * np.sum(theta * theta)
    grad = (-1 / m) * (indicator - prob_data).dot(data.transpose()) + lambda</em> * theta
    return cost, grad.flatten()
```</p>

<h2>C/C++ 代码</h2>

<pre><code class="cpp">#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;string.h&gt;
#include &lt;math.h&gt;
#include &lt;pthread.h&gt;

#define MAX_STRING 100
#define EXP_TABLE_SIZE 1000
#define MAX_EXP 6
#define MAX_SENTENCE_LENGTH 1000
#define MAX_CODE_LENGTH 40

const int vocab_hash_size = 30000000;  // Maximum 30 * 0.7 = 21M words in the vocabulary

typedef float real;                    // Precision of float numbers

struct vocab_word {
  long long cn;
  int *point;
  char *word, *code, codelen;
};
void InitUnigramTable() {
  int a, i;
  double train_words_pow = 0;
  double d1, power = 0.75;
  table = (int *)malloc(table_size * sizeof(int));
  for (a = 0; a &lt; vocab_size; a++) 
    train_words_pow += pow(vocab[a].cn, power);
  i = 0;
  d1 = pow(vocab[i].cn, power) / train_words_pow;
  for (a = 0; a &lt; table_size; a++) {
    table[a] = i;
    if (a / (double)table_size &gt; d1) {
      i++;
      d1 += pow(vocab[i].cn, power) / train_words_pow;
    }
    if (i &gt;= vocab_size) i = vocab_size - 1;
  }
}
</code></pre>

<h2>Shell</h2>

<pre><code class="sh">#!/bin/bash
# Program:
#       This program shows "Hello World!" in your screen.
# History:
# 2005/08/23    VBird   First release
PATH=/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/bin:/usr/local/sbin:~/bin
export PATH
echo -e "Hello World! \a \n"
exit 0
</code></pre>

<h2>我的爱好</h2>

<h3>Alizee1</h3>

<p><img src="/images/alizee1.jpg"></p>

<h3>Alizee2</h3>

<p><img src="/images/alizee2.jpg"></p>
]]></content>
  </entry>
  
</feed>
